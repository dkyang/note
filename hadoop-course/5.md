# 第八讲：hdfs详细分析三（四）（数据压缩）
* hadoop-native库
* CodecCompression
* java反射机制
* zlib，snappy
通过反射完成压缩：
```
String codecClassName = "org.apache.hadoop.io.compress.DefaultCodec";
Class<?> cls = Class.forName(codecClassName);
Configuration conf = new Configuration();
codec = (CompressionCodec)ReflectionUtils.new
CompressionOutputStream oout = codec.createOutputStream(fileOut);
String inputFile = "/home/louis/hadoop/data";
String outFile = inputFile + codec.getDefaultExtension();
FileOutputStream fileOut = new FileOutputStream(outFile);
CompressionOutputStream out = codec.createOutputStream(fileOut);
IOUtils.copyBytes(in, out, 4096, false);
in.close();
out.close();

```

* -Djava.library.path=(native-hadoop);/usr/local/lib*(native-snappy)
* ll -h

1) 常见数据压缩算法
deflate, bzip2, gzip, snappy

2) native压缩库算法
snappy -> libsnappy.so libhadoop.so
gzip, deflate -> zlib libhadoop.so

3) CompressionCodec
压缩：CreateOutStream 来得到CompressionOutputStream
解压：CreateInputStream 来的到CompressionInputStream

```
String file = "/home/louis/hadoop/README.txt.gz";
Configuration conf = new Configuration();
CompressionCodecFactory codecFactory = new CompressionCodecFactory(conf);
CompressionCodec codecFactory.getCodec(new Path(file));
CompressionInputStream in = codec.createInputStream(new FileInputStream(new FileInputStream(new File(file)));
FileOutputStream out = new FileOutputStream(new File(codecFactory.removeSuffix(file, codec.getDefaultExtension()));
IOUtils.copyBytes(in, out, 4096);
