第四讲：hdfs详细分析一
# 常见分布式文件系统
* fastDFS：以文件为基本存储单位。storage cluster分成若干组，同一个组内文件同步，每一组内机器最好同构（木桶效应）。
* MooseFS：master server、chunk server、client。 client有fuse接口，可将分布式文件系统mount到本地，使用起来像本地文件系统。
* 以文件为基本存储单位：1. 难以并行化，一个节点处理一个文件。2. 文件大小各不相同，难以负载均衡。

# HDFS
* HDFS架构：namenode——元数据信息，datanode——副本
* 读过程：
DistributedFileSystem -> FSDataInputStream -> DFSClient.open(RPC通信机制) -> NN.open()(namenode)      

向namenode发出请求，通过FSData InputStream从最近的datanode中读信息。
client保存了namenode的动态代理对象，远程RPC
* 写过程：
DistributedFileSystem -> FSDataOutputStream -> DFSClient.create(RPC通信机制) -> NN.create

通过open()创建Distributed FileSystem，以rpc方式连接namenode。FSDataOutputStream顺序向DataNode写。
* 通过eclipse阅读源码：create top-level code（common、hdfs）
* 读写均通过RCP，两端均继承通信协议。NameNode继承了三个协议：ClientProtocol、DatanodeProtocol、NamenodeProtocol。
ClientProtocol——client与namenode之间
DatanodeProtocol——datanode与namenode
NamenodeProtocol——namenode与secondary namenode
通讯双方都要实现协议的接口，client端保存对端的动态代理对象

# SecondNameNode
* 不是完全意义上namenode的备份
* 从namenode上拉去Fsimage和edits文件，在SNN的内存中进行合并（触发checkpoint）
* 周期性合并fsimage和editslog，并推送给namenode。fsimage是namenode的镜像，editslog记录对fsimage操作的步骤。
* 辅助恢复namenode
* SecondaryNameNode被checkpoint node和backup node替换。checkpoint node功能完全和SecondaryNameNode相同，
backup node是checkpoint node的完全备份。
* SecondaryNamenode通过fsimage和editslog合并，减小了namenode中可能的edistlog大小的不断增大。

需要在配置文件中配置：
1. fs.checkpoint.period 检查周期
2. fs.checkpoint.size editslog大小达到这个数值时，就会触发checkpoint
3. fs.checkpoint.dir SNN的editslog、fsimage存放位置

VERSION文件保存了HDFS的版本号；layoutVersion是一个负整数，保存了HDFS的持续化在硬盘上数据结构的版本号。
namespaceID是文件系统的唯一标识符，文件系统初始化时生成。storageType表示文件夹中保存的是元数据节点的数据结构。

一旦NN或者元数据信息丢失，我们可以通过SNN的检查点目录恢复我们的元数据信息
* hadoop namenode -importCheckpoint
* hadoop-daemon.sh start namenode

# CheckPoint Node 
CheckPointNode与SecondaryNameNode的功能、配置属性完全相同，但只能出现在2.0或者0.23版本中。
# Backup Node
* 完全意义上的namenode备份节点（完全的元数据信息，SNN只是在内存中拉取合并）
* Backup Node在内存中维护了一份从Namenode同步过来的fsimage，同时从namenode接收edits文件的日志流，并持久化到硬盘。
而SecondaryNamenode只是在内存中进行fsimage和editslog的合并，并不真正意义上提供备份。
# 通过SecondaryNameNode的check point恢复namenode
* 制造故障：kill -9 pid（namenode）， 删除namenode元数据（dfs.name.dir）
* ./hadoop namenode -importCheckpoint 从check point获取fsimage和edit log的合并信息
* ./hadoop-daemon.sh start namenode 重启namenode
# HDFS副本放置策略
第一个block副本放在与client端离得最近的node中，第二个block副本放在与第一个不同机架的node中，第三个block副本放在第二个副本同一机架的不同node中。
# 机架感知
* 通过机架感知，namenode能得到datanode的网络拓扑结构，从而可以计算得到一个节点到另一个节点的距离。
* 若未配置机架信息，所有机器默认在同一个机架下。通过配置toplogy.script.file.name按照网络拓扑结构寻找datanode，其属性值为一个脚本（perl python），
这个脚本里面写得是真正意义上的网络拓扑结构。/d1/rack1/dn1
* 默认为/default-rack
